{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport pandas_datareader as pdr\nfrom datetime import datetime\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn import preprocessing\nfrom pandas import concat\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import LSTM\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom numpy import concatenate"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["ge = pdr.get_data_yahoo('GE', datetime(2000, 1, 1), datetime(2018, 1, 1))\naapl = pdr.get_data_yahoo('AAPL', datetime(2000, 1, 1), datetime(2018, 1, 1))\nfb = pdr.get_data_yahoo('FB', datetime(2012, 6, 6), datetime(2018, 1, 1))\ngs = pdr.get_data_yahoo('GS', datetime(2000, 1, 1), datetime(2018, 1, 1))\nbtc = pdr.get_data_yahoo('BTC-USD', datetime(2011, 1, 1), datetime(2018, 1, 1))\nfb.head()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["#data preprocessing - dropping columns Close, Volume. Adj Close will be the Y variable\nge.drop(['Close','Volume'],axis =1, inplace = True)\naapl.drop(['Close','Volume'],axis =1, inplace = True)\nfb.drop(['Close','Volume'],axis =1, inplace = True)\ngs.drop(['Close','Volume'],axis =1, inplace = True)\nbtc.drop(['Close','Volume'],axis =1, inplace = True)\nfb.head()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["plt.plot(fb[\"Adj Close\"])\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#LSTMs are sensitive to data scales when activation functions are used.Its a good practise to range the data between 0,1\n#Normalising using MinMaxscaler\n#FB\nfb_values = fb.values.astype('float32')\nfb_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\nscaled_fb = fb_scaler.fit_transform(fb_values)\nprint(scaled_fb)\n#GE\nge_values = ge.values.astype('float32')\nge_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\nscaled_ge = ge_scaler.fit_transform(ge_values)\n#AAPL\naapl_values = aapl.values.astype('float32')\naapl_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\nscaled_aapl = aapl_scaler.fit_transform(aapl_values)\n#GS\ngs_values = gs.values.astype('float32')\ngs_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\nscaled_gs = gs_scaler.fit_transform(gs_values)\n#BTC\nbtc_values = btc.values.astype('float32')\nbtc_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\nscaled_btc = btc_scaler.fit_transform(btc_values)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["fb_cols,ge_cols,aapl_cols,gs_cols,btc_cols, cols_names = list(),list(),list(),list(),list(), list()\nfeatures = scaled_fb.shape[1] #Number of features. same for all\nfb_df = pd.DataFrame(scaled_fb)\nge_df = pd.DataFrame(scaled_ge)\naapl_df = pd.DataFrame(scaled_aapl)\ngs_df = pd.DataFrame(scaled_gs)\nbtc_df = pd.DataFrame(scaled_btc)\n\n#Timestep is the number of previous time steps to be used as input to predict the next time step\n#We can change values to see which one gives the best results for our model\ninput_ts = 25\n\n#Based on the value of the timestep variable 'input_ts', values at t-n, ... t-1 are appended to the dataset \n#for all the features to create an input sequence\n#Hence, one input sequence will have (input_ts * features) number of variables \nfor i in range(input_ts, 0, -1):\n  fb_cols.append(fb_df.shift(i))\n  ge_cols.append(ge_df.shift(i))\n  aapl_cols.append(aapl_df.shift(i))\n  gs_cols.append(gs_df.shift(i))\n  btc_cols.append(btc_df.shift(i))\n  cols_names += [('feature%d(t-%d)' % (j+1, i)) for j in range(features)]\n  \n#Sequence (t) is also appended which will be our output timestep, ie, the value of timestep 't' is \n#predicted using the data for timesteps from (t-1) to (t-input_timeSteps)\nfb_cols.append(fb_df.shift(0))\nge_cols.append(ge_df.shift(0))\naapl_cols.append(aapl_df.shift(0))\ngs_cols.append(gs_df.shift(0))\nbtc_cols.append(btc_df.shift(0))\n\ncols_names += [('feature%d(t)' % (j+1)) for j in range(features)]\n\nfb_ModifiedSeq = concat(fb_cols, axis=1)\nge_ModifiedSeq = concat(ge_cols, axis=1)\naapl_ModifiedSeq = concat(aapl_cols, axis=1)\ngs_ModifiedSeq = concat(gs_cols, axis=1)\nbtc_ModifiedSeq = concat(btc_cols, axis=1)\nfb_ModifiedSeq .columns = cols_names\nge_ModifiedSeq .columns = cols_names\naapl_ModifiedSeq .columns = cols_names\ngs_ModifiedSeq .columns = cols_names\nbtc_ModifiedSeq .columns = cols_names\nfb_ModifiedSeq .dropna(inplace=True) # Omitting rows with any of the missing values\nge_ModifiedSeq .dropna(inplace=True)\naapl_ModifiedSeq .dropna(inplace=True)\ngs_ModifiedSeq .dropna(inplace=True)\nbtc_ModifiedSeq .dropna(inplace=True)\nprint(fb_ModifiedSeq)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#####################################Training and Test data sets#############################################################\n#We cannot use cross validation method here to validate our model because sequence is important in time series.##############\n#Instead, we can split our past data into train data and test data.##########################################################"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#Using 90% of the data for training and 10% for testing\nfb_train_size = int(len(fb_ModifiedSeq.values) * 0.9)\nfb_test_size = len(fb_ModifiedSeq.values) - fb_train_size\nfb_train, fb_test = fb_ModifiedSeq.values[0:fb_train_size,:], fb_ModifiedSeq.values[fb_train_size:len(fb_ModifiedSeq.values),:]\n\nge_train_size = int(len(ge_ModifiedSeq.values) * 0.9)\nge_test_size = len(ge_ModifiedSeq.values) - ge_train_size\nge_train, ge_test = ge_ModifiedSeq.values[0:ge_train_size,:], ge_ModifiedSeq.values[ge_train_size:len(ge_ModifiedSeq.values),:]\n\naapl_train_size = int(len(aapl_ModifiedSeq.values) * 0.9)\naapl_test_size = len(aapl_ModifiedSeq.values) - aapl_train_size\naapl_train, aapl_test = aapl_ModifiedSeq.values[0:aapl_train_size,:], aapl_ModifiedSeq.values[aapl_train_size:len(aapl_ModifiedSeq.values),:]\n\ngs_train_size = int(len(gs_ModifiedSeq.values) * 0.9)\ngs_test_size = len(gs_ModifiedSeq.values) - gs_train_size\ngs_train, gs_test = gs_ModifiedSeq.values[0:gs_train_size,:], gs_ModifiedSeq.values[gs_train_size:len(gs_ModifiedSeq.values),:]\n\nbtc_train_size = int(len(btc_ModifiedSeq.values) * 0.9)\nbtc_test_size = len(btc_ModifiedSeq.values) - btc_train_size\nbtc_train, btc_test = btc_ModifiedSeq.values[0:btc_train_size,:], btc_ModifiedSeq.values[btc_train_size:len(btc_ModifiedSeq.values),:]\n\nprint(len(fb_train), len(fb_test))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["fb_train"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#Out of the total ((input_ts * features) + features) columns added by the above steps, the first (input_ts * features) columns\n#that correspond to the timesteps (t-1) to (t-input_ts) will be our input features. Our prediction variable,\n#which is the last column (that corresponds to 'close' price at timestep (t)) will be our 'y' variable.\nfb_trainX = fb_train[:, :features*input_ts] \nfb_trainY = fb_train[:,-1]\nge_trainX = ge_train[:, :features*input_ts] \nge_trainY = ge_train[:,-1]\naapl_trainX = aapl_train[:, :features*input_ts] \naapl_trainY = aapl_train[:,-1]\ngs_trainX = gs_train[:, :features*input_ts] \ngs_trainY = gs_train[:,-1]\nbtc_trainX = btc_train[:, :features*input_ts] \nbtc_trainY = btc_train[:,-1]\n    \nfb_testX = fb_test[:, :features*input_ts] \nfb_testY = fb_test[:,-1]\nge_testX = ge_test[:, :features*input_ts] \nge_testY = ge_test[:,-1]\naapl_testX = aapl_test[:, :features*input_ts] \naapl_testY = aapl_test[:,-1]\ngs_testX = gs_test[:, :features*input_ts] \ngs_testY = gs_test[:,-1]\nbtc_testX = btc_test[:, :features*input_ts] \nbtc_testY = btc_test[:,-1]\n \n#Input is reshaped to 3D [Number of rows, timesteps, Number of features] . Input requirement for LSTM   \nfb_trainX = np.reshape(fb_trainX,(fb_trainX.shape[0], input_ts,features))\nge_trainX = np.reshape(ge_trainX,(ge_trainX.shape[0], input_ts,features))\naapl_trainX = np.reshape(aapl_trainX,(aapl_trainX.shape[0], input_ts,features))\ngs_trainX = np.reshape(gs_trainX,(gs_trainX.shape[0], input_ts,features))\nbtc_trainX = np.reshape(btc_trainX,(btc_trainX.shape[0], input_ts,features))\n\nfb_testX = np.reshape(fb_testX,(fb_testX.shape[0], input_ts, features))\nge_testX = np.reshape(ge_testX,(ge_testX.shape[0], input_ts, features))\naapl_testX = np.reshape(aapl_testX,(aapl_testX.shape[0], input_ts, features))\ngs_testX = np.reshape(gs_testX,(gs_testX.shape[0], input_ts, features))\nbtc_testX = np.reshape(btc_testX,(btc_testX.shape[0], input_ts, features))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["############################################# Build and fit LSTM Model #############################################"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#Build the LSTM model\n#FB\nfb_model = Sequential()\nfb_model.add(LSTM(260, input_shape=(fb_trainX.shape[1], fb_trainX.shape[2]), return_sequences=True, activation='linear')) #stack 2 lstm s together\nfb_model.add(LSTM(260, input_shape=(fb_trainX.shape[1], fb_trainX.shape[2]), return_sequences=False, activation='linear'))\nfb_model.add(Dense(1))\nfb_model.compile(loss='mse', optimizer='adam') #keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n#Fit our model on the Training Set\nret = fb_model.fit(fb_trainX, fb_trainY, epochs=100, batch_size=50, validation_split=0.2, verbose=2, shuffle=False)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#GE\nge_model = Sequential()\nge_model.add(LSTM(300, input_shape=(ge_trainX.shape[1], ge_trainX.shape[2]), return_sequences=True, activation='linear')) #stack 2 lstm s together\nge_model.add(LSTM(300, input_shape=(ge_trainX.shape[1], ge_trainX.shape[2]), return_sequences=False, activation='linear'))\nge_model.add(Dense(1))\nge_model.compile(loss='mse', optimizer='adam') #keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n#Fit our model on the Training Set\nge_ret = ge_model.fit(ge_trainX, ge_trainY, epochs=80, batch_size=200, validation_split=0.2, verbose=2, shuffle=False)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#AAPL\naapl_model = Sequential()\naapl_model.add(LSTM(350, input_shape=(aapl_trainX.shape[1], aapl_trainX.shape[2]), return_sequences=True, activation='linear')) #stack 2 lstm s together\naapl_model.add(LSTM(120, input_shape=(aapl_trainX.shape[1], aapl_trainX.shape[2]), return_sequences=False, activation='linear'))\naapl_model.add(Dense(1))\naapl_model.compile(loss='mse', optimizer='adam') #keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n#Fit our model on the Training Set\naapl_ret = aapl_model.fit(aapl_trainX, aapl_trainY, epochs=100, batch_size=250, validation_split=0.2, verbose=2, shuffle=False)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#GS\ngs_model = Sequential()\ngs_model.add(LSTM(265, input_shape=(gs_trainX.shape[1], gs_trainX.shape[2]), return_sequences=True, activation='linear')) #stack 2 lstm s together\ngs_model.add(LSTM(260, input_shape=(gs_trainX.shape[1], gs_trainX.shape[2]), return_sequences=False, activation='linear'))\ngs_model.add(Dense(1))\ngs_model.compile(loss='mse', optimizer='adam') #keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n#Fit our model on the Training Set\ngs_ret = gs_model.fit(gs_trainX, gs_trainY, epochs=100, batch_size=150, validation_split=0.2, verbose=2, shuffle=False)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#BTC\nbtc_model = Sequential()\nbtc_model.add(LSTM(280, input_shape=(btc_trainX.shape[1], btc_trainX.shape[2]), return_sequences=True, activation='linear')) #stack 2 lstm s together\nbtc_model.add(LSTM(280, input_shape=(btc_trainX.shape[1], btc_trainX.shape[2]), return_sequences=False, activation='linear'))\nbtc_model.add(Dense(1))\nbtc_model.compile(loss='mse', optimizer='adam') #keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n#Fit our model on the Training Set\nbtc_ret = btc_model.fit(btc_trainX, btc_trainY, epochs=100, batch_size=250, validation_split=0.2, verbose=2, shuffle=False)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#model.fit() method returns a History object. Its History.history attribute records training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values\n#FB\nfig = plt.figure()\nplt.plot(ret.history['loss'],label='Train Loss')\nplt.plot(ret.history['val_loss'],label='Test Loss')\nplt.title('FB - Loss at successive epochs')\nplt.legend(loc='best')\ndisplay(plt.show())\nplt.close(fig)\n\n#From the plot, we can see that the model has comparable performance on both train and validation datasets."],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["#GE\nfig = plt.figure()\nplt.plot(ge_ret.history['loss'],label='Train Loss')\nplt.plot(ge_ret.history['val_loss'],label='Test Loss')\nplt.title('GE - Loss at successive epochs')\nplt.legend(loc='best')\ndisplay(plt.show())\nplt.close(fig)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["#AAPL\nfig = plt.figure()\nplt.plot(aapl_ret.history['loss'],label='Train Loss')\nplt.plot(aapl_ret.history['val_loss'],label='Test Loss')\nplt.title('AAPL - Loss at successive epochs')\nplt.legend(loc='best')\ndisplay(plt.show())\nplt.close(fig)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["#GS\nfig = plt.figure()\nplt.plot(gs_ret.history['loss'],label='Train Loss')\nplt.plot(gs_ret.history['val_loss'],label='Test Loss')\nplt.title('GS - Loss at successive epochs')\nplt.legend(loc='best')\ndisplay(plt.show())\nplt.close(fig)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["#BTC\nfig = plt.figure()\nplt.plot(btc_ret.history['loss'],label='Train Loss')\nplt.plot(btc_ret.history['val_loss'],label='Test Loss')\nplt.title('BTC - Loss at successive epochs')\nplt.legend(loc='best')\ndisplay(plt.show())\nplt.close(fig)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# We scaled our dataset before feeding to the LSTM. As a result, our prediction variable also has a scaled value. Therefore, we should invert it to the orginal form and then compare with the actual value to get the rmse. This will give us an error measurement in the same unit as the original variable.\n\n#Make predictions for our test data\nfb_predictions = fb_model.predict(fb_testX)\nge_predictions = ge_model.predict(ge_testX)\naapl_predictions = aapl_model.predict(aapl_testX)\ngs_predictions = gs_model.predict(gs_testX)\nbtc_predictions = btc_model.predict(btc_testX)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["fb_testY"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["#Invert the scales for predictions\nfb_testX = fb_testX.reshape((fb_testX.shape[0], input_ts*features))\nfb_combined1 = concatenate((fb_predictions, fb_testX[:, -3:]), axis=1)\nfb_inverted_predictions = fb_scaler.inverse_transform(fb_combined1)[:,0]\n\nge_testX = ge_testX.reshape((ge_testX.shape[0], input_ts*features))\nge_combined1 = concatenate((ge_predictions, ge_testX[:, -3:]), axis=1)\nge_inverted_predictions = ge_scaler.inverse_transform(ge_combined1)[:,0]\n\naapl_testX = aapl_testX.reshape((aapl_testX.shape[0], input_ts*features))\naapl_combined1 = concatenate((aapl_predictions, aapl_testX[:, -3:]), axis=1)\naapl_inverted_predictions = aapl_scaler.inverse_transform(aapl_combined1)[:,0]\n\ngs_testX = gs_testX.reshape((gs_testX.shape[0], input_ts*features))\ngs_combined1 = concatenate((gs_predictions, gs_testX[:, -3:]), axis=1)\ngs_inverted_predictions = gs_scaler.inverse_transform(gs_combined1)[:,0]\n\nbtc_testX = btc_testX.reshape((btc_testX.shape[0], input_ts*features))\nbtc_combined1 = concatenate((btc_predictions, btc_testX[:, -3:]), axis=1)\nbtc_inverted_predictions = btc_scaler.inverse_transform(btc_combined1)[:,0]\n\n#Invert the scales for original data\nfb_testY = fb_testY.reshape((len(fb_testY), 1))\nfb_combined2 = concatenate((fb_testY, fb_testX[:, -3:]), axis=1)\nfb_inverted_original = fb_scaler.inverse_transform(fb_combined2)[:,0]\n\nge_testY = ge_testY.reshape((len(ge_testY), 1))\nge_combined2 = concatenate((ge_testY, ge_testX[:, -3:]), axis=1)\nge_inverted_original = ge_scaler.inverse_transform(ge_combined2)[:,0]\n\naapl_testY = aapl_testY.reshape((len(aapl_testY), 1))\naapl_combined2 = concatenate((aapl_testY, aapl_testX[:, -3:]), axis=1)\naapl_inverted_original = aapl_scaler.inverse_transform(aapl_combined2)[:,0]\n\ngs_testY = gs_testY.reshape((len(gs_testY), 1))\ngs_combined2 = concatenate((gs_testY, gs_testX[:, -3:]), axis=1)\ngs_inverted_original = gs_scaler.inverse_transform(gs_combined2)[:,0]\n\nbtc_testY = btc_testY.reshape((len(btc_testY), 1))\nbtc_combined2 = concatenate((btc_testY, btc_testX[:, -3:]), axis=1)\nbtc_inverted_original = btc_scaler.inverse_transform(btc_combined2)[:,0]\n"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["\n#Calculate RMSE\nfb_rmse = sqrt(mean_squared_error(fb_inverted_original, fb_inverted_predictions))\nprint('FB - Test RMSE: %.3f' % fb_rmse)\nge_rmse = sqrt(mean_squared_error(ge_inverted_original, ge_inverted_predictions))\nprint('GE - Test RMSE: %.3f' % ge_rmse)\naapl_rmse = sqrt(mean_squared_error(aapl_inverted_original, aapl_inverted_predictions))\nprint('AAPL - Test RMSE: %.3f' % aapl_rmse)\ngs_rmse = sqrt(mean_squared_error(gs_inverted_original, gs_inverted_predictions))\nprint('GS - Test RMSE: %.3f' % gs_rmse)\nbtc_rmse = sqrt(mean_squared_error(btc_inverted_original, btc_inverted_predictions))\nprint('BTC - Test RMSE: %.3f' % btc_rmse)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["#FB\nfig = plt.figure()\nplt.plot(fb_inverted_predictions, color='red', label='Prediction')\nplt.plot(fb_inverted_original, color='blue', label='Original')\nplt.legend(loc='best')\nplt.title(\"FB - Predictions Vs Original\")\ndisplay(plt.show())\nplt.close(fig)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["#GE\nfig = plt.figure()\nplt.plot(ge_inverted_predictions, color='red', label='Prediction')\nplt.plot(ge_inverted_original, color='blue', label='Original')\nplt.legend(loc='best')\nplt.title(\"GE - Predictions Vs Original\")\ndisplay(plt.show())\nplt.close(fig)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["#AAPL\nfig = plt.figure()\nplt.plot(aapl_inverted_predictions, color='red', label='Prediction')\nplt.plot(aapl_inverted_original, color='blue', label='Original')\nplt.legend(loc='best')\nplt.title(\"AAPL - Predictions Vs Original\")\ndisplay(plt.show())\nplt.close(fig)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["#GS\nfig = plt.figure()\nplt.plot(gs_inverted_predictions, color='red', label='Prediction')\nplt.plot(gs_inverted_original, color='blue', label='Original')\nplt.legend(loc='best')\nplt.title(\"GS - Predictions Vs Original\")\ndisplay(plt.show())\nplt.close(fig)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["#BTC\nfig = plt.figure()\nplt.plot(btc_inverted_predictions, color='red', label='Prediction')\nplt.plot(btc_inverted_original, color='blue', label='Original')\nplt.legend(loc='best')\nplt.title(\"BTC - Predictions Vs Original\")\ndisplay(plt.show())\nplt.close(fig)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":31}],"metadata":{"name":"TimeSeriesLSTM","notebookId":4109360145566906},"nbformat":4,"nbformat_minor":0}
